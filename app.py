import streamlit as st
import google.generativeai as genai
import glob
import time

# --- 1. CONFIGURATION ---
st.set_page_config(page_title="Scholars HQ", page_icon="üéì")
st.title("üéì Scholars HQ: Franchise Support")

# Securely fetch the API key
try:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
except Exception:
    st.error("Missing API Key. Please configure it in Streamlit Secrets.")

# --- 2. INTELLIGENT LOADING (CACHED) ---
# This @st.cache_resource line stops the app from re-reading files on every click
@st.cache_resource
def load_knowledge():
    """Reads all .txt files in the repository and combines them."""
    combined_text = ""
    # Looks for any text file generated by your manager script
    files = glob.glob("*.txt")
    
    if not files:
        return "No specific internal documents found."
        
    doc_count = 0
    for filename in files:
        try:
            with open(filename, "r", encoding="utf-8") as f:
                content = f.read()
                # We add a clear header so the AI knows where the text comes from
                combined_text += f"\n\n--- SOURCE DOCUMENT: {filename} ---\n{content}"
                doc_count += 1
        except Exception as e:
            print(f"Skipping {filename}: {e}")
            
    return combined_text, doc_count

# Load data
knowledge_base_text, count = load_knowledge()

# Show a tiny status indicator
if count > 0:
    st.success(f"Brain loaded with {count} documents.", icon="üß†")

# --- 3. THE PERSONA ---
system_instruction = f"""
You are 'Scholars HQ', the central support AI for Scholars Education.
Your goal is to support new franchisees with operational, curricular, and historical inquiries.

CORE KNOWLEDGE:
- Origin: Founded in 1999, based in Toronto, Canada.
- Tone: Professional, encouraging, corporate but accessible.
- If asked about fees/royalties, be precise based on the documents.

REFERENCE MATERIAL:
Use the following internal documents to answer questions. 
{knowledge_base_text}
"""

# --- 4. CHAT LOGIC ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({
        "role": "model", 
        "content": "Hello. I have read the internal files. How can I help?"
    })

# Display Chat History
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Handle User Input
if prompt := st.chat_input("Ask Scholars HQ..."):
    
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("model"):
        try:
            # We use gemini-1.5-flash because it has the highest "Tokens Per Minute" limit
            model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=system_instruction)
            
            # Send message
            chat = model.start_chat(history=[
                {"role": m["role"], "parts": m["content"]} 
                for m in st.session_state.messages[:-1]
            ])
            
            response = chat.send_message(prompt)
            st.markdown(response.text)
            st.session_state.messages.append({"role": "model", "content": response.text})

        except Exception as e:
            # If we hit the limit, we catch the error gracefully
            if "ResourceExhausted" in str(e):
                st.error("‚ö†Ô∏è Traffic Jam! We hit the speed limit (Quota). Please wait 60 seconds and try again.")
            else:
                st.error(f"An error occurred: {e}")